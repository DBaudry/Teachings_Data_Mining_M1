{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective of this practical session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why unsupervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last sessions we learned to handle labelled dataset properly and answer simple regression/classification problems, with a simple procedure:\n",
    "    1. Cleaning/Data preparation: remove missing values\n",
    "    2. Feature Selection/Feature Extraction with adapted statistics and     visualization tools\n",
    "    3. Choice of some appropriate families models to test\n",
    "    4. Parameter tuning for each kind of model\n",
    "    5. Back to step 2) if the best models are not satisfying\n",
    "\n",
    "\n",
    "However, you've seen in class that in many real world applications the dataset does not come with proper classes and the labelling in itself is the core of the problem. For example in finance one may want to define market regimes (are we in a crisis or in a financial bubble?) or to perform asset segmentation according to some properties of the history of the returns of each asset (identify trend following assets, pairs, etc...). \n",
    "\n",
    "In practice most of the machine learning models used in market finance are unsupervised models due to the lack of \"ground truth\" when looking at the current situation or forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall the unsupervised classification framework: starting from samples $(X_1,\\dots X_n)$, where for $i\\in \\{1,\\dots n\\}$, $X_i$ can be a vector of quantitative or qualitative variables, we will try to define a partition of $\\{1,\\dots n\\}$ where individuals in the same element of the partition are \"close\" in some sense we need to define.\n",
    "<br><br> \n",
    "\n",
    "Due to the lack of ground truth in real world datasets we will mainly work with simulated data, in order to understand the behaviour of each algorithm in our examples. <br><br> \n",
    "\n",
    "\n",
    "Based on these simulated data, we will perform the two simples unsupervised algorithms: $k$-means and hierarchical clustering. An optional section will allow students who want to go further to experiment Spectral clustering, which shows that more sophisticated methods exist. Our objective will be to understand for which kind of data each method can work, and why. <br><br> \n",
    "\n",
    "\n",
    "\n",
    "At the hand on the notebook we will work with a legend of Machine Learning: the MNIST dataset. https://fr.wikipedia.org/wiki/Base_de_donn%C3%A9es_MNIST\n",
    "<br>\n",
    "\n",
    "\n",
    "This dataset contains pictures of handwritten digits, and our objective will be to predict this number with an unsupervised model. We will figure that even though this method is far from the state of the art from this dataset a simple PCA does quite a good job in this problem. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Note:** this Notebook is yours and you are warmly encouraged to play with different settings, write some comments, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Why-unsupervised-learning?\" data-toc-modified-id=\"Why-unsupervised-learning?-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Why unsupervised learning?</a></span></li><li><span><a href=\"#Content-of-this-notebook\" data-toc-modified-id=\"Content-of-this-notebook-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Content of this notebook</a></span></li><li><span><a href=\"#How-to-generate-your-toy-example\" data-toc-modified-id=\"How-to-generate-your-toy-example-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>How to generate your toy example</a></span><ul class=\"toc-item\"><li><span><a href=\"#k-Means\" data-toc-modified-id=\"k-Means-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>k-Means</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualization-of-k-Means-step-by-step\" data-toc-modified-id=\"Visualization-of-k-Means-step-by-step-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Visualization of k-Means step-by-step</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mesh-the-input-space-to-display-the-decision-boundary\" data-toc-modified-id=\"Mesh-the-input-space-to-display-the-decision-boundary-3.1.1.1\"><span class=\"toc-item-num\">3.1.1.1&nbsp;&nbsp;</span>Mesh the input space to display the decision boundary</a></span></li></ul></li><li><span><a href=\"#evaluation-Rand-index-vs-ARI-(Adjusted-Rand-Index)\" data-toc-modified-id=\"evaluation-Rand-index-vs-ARI-(Adjusted-Rand-Index)-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>evaluation Rand index vs ARI (Adjusted Rand Index)</a></span></li><li><span><a href=\"#optimization-of-the-number-of-clusters\" data-toc-modified-id=\"optimization-of-the-number-of-clusters-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>optimization of the number of clusters</a></span></li></ul></li><li><span><a href=\"#Limites-of-k-means-algorithm\" data-toc-modified-id=\"Limites-of-k-means-algorithm-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Limites of k-means algorithm</a></span></li></ul></li><li><span><a href=\"#Hierarchical-clustering\" data-toc-modified-id=\"Hierarchical-clustering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Hierarchical clustering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-dendogram\" data-toc-modified-id=\"Visualize-dendogram-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Visualize dendogram</a></span></li></ul></li><li><span><a href=\"#Spectral-clustering-(Facultative-part,-Bonus)\" data-toc-modified-id=\"Spectral-clustering-(Facultative-part,-Bonus)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Spectral clustering (Facultative part, Bonus)</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Create-the-similarity-graph\" data-toc-modified-id=\"Create-the-similarity-graph-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;</span>Create the similarity graph</a></span></li></ul></li></ul></li><li><span><a href=\"#PCA\" data-toc-modified-id=\"PCA-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>PCA</a></span><ul class=\"toc-item\"><li><span><a href=\"#MNIST\" data-toc-modified-id=\"MNIST-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>MNIST</a></span></li></ul></li><li><span><a href=\"#Dimensionality-reduction-with-PCA\" data-toc-modified-id=\"Dimensionality-reduction-with-PCA-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Dimensionality reduction with PCA</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Visualization-of-some-handwritten-digits\" data-toc-modified-id=\"Visualization-of-some-handwritten-digits-7.0.1\"><span class=\"toc-item-num\">7.0.1&nbsp;&nbsp;</span>Visualization of some handwritten digits</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:12:39.056053Z",
     "start_time": "2020-04-01T09:12:39.044765Z"
    }
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:12:40.094287Z",
     "start_time": "2020-04-01T09:12:39.451365Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To generate you favorite toy example\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "\n",
    "# Algorithms of the day\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "# k-Means\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Hierarchical clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, dendrogram\n",
    "\n",
    "# Spectral clustering with sklearn\n",
    "from sklearn.cluster import SpectralClustering\n",
    "# Spectral clustering by hand\n",
    "from sklearn.metrics.pairwise import rbf_kernel  # similarity metric\n",
    "import networkx as nx  # to play with graphs\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits, load_iris\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to generate your toy example\n",
    "\n",
    "`sklearn` allows you to build your favorite toy examples, click [here](https://scikit-learn.org/stable/modules/classes.html#samples-generator) for more details.\n",
    "\n",
    "In particular, if you wish to generate a mixture of isotropic gaussians you can use the `make_blobs` methods.\n",
    "\n",
    "Using the code from the last Python session, generate 300 independent Gaussian random vectors of $\\mathbb{R}^2$: 100 with expectation $(0,0)$, 100 with expectation $(-4,4)$ and 100 with expectation $(4,4)$, and all with covariance matrix equal to indentity. Plot the simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Visualization of k-Means step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the initialization step use the following `class_inertia` function to\n",
    "\n",
    "- associate match a point to the nearest centroid\n",
    "- compute the total inertia of the initial configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_inertia(X, centroids):\n",
    "    \"\"\"Compute the square distance of each data points to the different centroids and \n",
    "    return the indices of the corresponding clusters and the total inertia.\n",
    "    \"\"\"\n",
    "    \n",
    "    sq_dist = np.array([np.einsum('ij,ij->i', X - c, X - c) for c in centroids])\n",
    "    class_indices = np.argmin(sq_dist, axis=0)\n",
    "    inertia = np.sum(sq_dist[class_indices, range(sq_dist.shape[1])])\n",
    "    \n",
    "    return class_indices, inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mesh the input space to display the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for visualization\n",
    "x_min, x_max = X_toy[:, 0].min() - .5, X_toy[:, 0].max() + .5\n",
    "y_min, y_max = X_toy[:, 1].min() - .5, X_toy[:, 1].max() + .5\n",
    "\n",
    "x1, x2 = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                     np.linspace(y_min, y_max, 100))\n",
    "X_mesh = np.column_stack([x1.ravel(), x2.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read and Explain the following code\n",
    "2. Run it and comment the results\n",
    "3. In new cells change the number of clusters and comment on the results. Remember that in a real world problem you don't know the real number of cluster. Would you have been able to identify that the real number of clusters is 3 by looking at the partition? Why? If not, what number of clusters would have looked rationnal to you? (open question, no \"true\" answer, explain your reasonning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:20:56.856426Z",
     "start_time": "2020-04-01T09:20:56.848886Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clust = 3\n",
    "\n",
    "# Initial centroids\n",
    "centroids = np.column_stack([x_min + (x_max - x_min)*np.random.rand(n_clust),\n",
    "                             y_min + (y_max - y_min)*np.random.rand(n_clust)])\n",
    "# Initial inertia\n",
    "_, inertia = class_inertia(X_toy, centroids)\n",
    "y_mesh, _ = class_inertia(X_mesh, centroids)\n",
    "\n",
    "for step in range(7):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    plt.title('{} clusters, inertia = {:.2f}\\niteration {}'.format(n_clust,inertia, step))\n",
    "    # Display the points\n",
    "    ax.scatter(X_toy[:,0], X_toy[:,1], c=cols[y_toy])\n",
    "    # Display current centroids\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1],\n",
    "               marker='x', s=169, linewidths=3,\n",
    "               color='w', zorder=10)\n",
    "    # Display current decision boundary\n",
    "    ax.contourf(x1, x2, y_mesh.reshape(x1.shape),\n",
    "                alpha=0.4,\n",
    "                cmap=plt.cm.Paired)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Make a one step of k-Means starting from current centroids\n",
    "    kmeans = KMeans(n_clusters=n_clust,\n",
    "                    init=centroids,\n",
    "                    max_iter=1,\n",
    "                    n_init=1)\n",
    "    # Fit the model\n",
    "    kmeans.fit(X_toy)\n",
    "    # Extract new centroids and inertia\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    inertia = kmeans.inertia_\n",
    "    \n",
    "    # Predict the mesh\n",
    "    y_mesh = kmeans.predict(X_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for experiment with n_cluster ther than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for experiment with n_cluster ther than 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your comment in this cell*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation Rand index vs ARI (Adjusted Rand Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your $k$-means classifier to predict the classes. Compute the confusion matrix and the ARI.\n",
    "\n",
    "*Note: you don't need to implement anything, use the sklearn functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:24:54.418896Z",
     "start_time": "2020-04-01T09:24:54.416260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimization of the number of clusters\n",
    "\n",
    "Elbow rule: plot the inertia as a function of the number of clusters. Conclude.\n",
    "<br><br> \n",
    "*Indice: again, you don't have to compute it. Look at the attributes of your predictor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your conclusion here*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limites of k-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your conclusions so far should be that the k-means algorithm performs pretty well for our toy dataset build with a gaussian mixture. Intuitively, it is clear that by construction points from the same cluster will be \"close\" in a $\\mathbb{R}^2$ euclidian distance sense. \n",
    "\n",
    "<br> However, this is not always the case and even in $\\mathbb{R}^2$ you could have very different structures that could be more challenging for k-means.\n",
    "\n",
    "<br>In this section we will build new toy datasets and observe the behaviour of k-means on their structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build two new toy datasets with `make_moons` and `make_blobs`. Low effort here, use the default parameters of the sklearn functions. Plot the datasets as in the first section. Comment on the shape on the clusters, how could you characterize them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:35:41.393162Z",
     "start_time": "2020-04-01T09:35:41.390867Z"
    }
   },
   "outputs": [],
   "source": [
    "### Generate moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:35:50.077730Z",
     "start_time": "2020-04-01T09:35:50.075481Z"
    }
   },
   "outputs": [],
   "source": [
    "### Generate circles dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment here on the charateristics of the clusters in each dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the code from the previous parts to run K-Means on these new datasets. Comment on the results: are they satisfying? Why? If not, is it *possible* to make them satisfying by transforming the data? How? (do not implement it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:38:59.168024Z",
     "start_time": "2020-04-01T09:38:59.165906Z"
    }
   },
   "outputs": [],
   "source": [
    "### K-means on moons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T09:39:09.623492Z",
     "start_time": "2020-04-01T09:39:09.617439Z"
    }
   },
   "outputs": [],
   "source": [
    "### K-means on circle dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answers here*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall how the method recursively clusters the population. Describe the different types of distances that can be chosen for merging existing clusters (you can use sklearn documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Answer here *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use `AgglomerativeClustering` to cluster the toy dataset.\n",
    "2. Display the corresponding clustering.\n",
    "3. Investigate the different parameters of `AgglomerativeClustering` in particular the `linkage` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Agglomerative clustering and displays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Other cells for parameters investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize dendogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the dendogram for the hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(X_toy, method='ward')\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('sample index or (cluster size)')\n",
    "plt.ylabel('distance')\n",
    "\n",
    "dendrogram(Z,\n",
    "           truncate_mode='lastp',  # show only the last p merged clusters\n",
    "           p=3,  # show only the last p merged clusters\n",
    "           leaf_font_size=12,\n",
    "           show_contracted=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fcluster(Z, 3, criterion='maxclust') - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_toy[:,0], X_toy[:,1], c=cols[y_pred], cmap='prism')  # plot points with cluster dependent colors\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Warning : the following section is facultative, but the sections following it are not. You are free to skip this one but don't forget to go to the next ones***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral clustering (Facultative part, Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the affinity\n",
    "$K_{ij} \\propto \\exp(-\\gamma \\|x_i - x_j\\|^2)$ \n",
    "between all pairs of points.\n",
    "\n",
    "1. Use `rbf_kernel` to compute the corresponding `affinity_matrix` using \n",
    "2. Display the affinity matrix using `plt.matshow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_matrix = rbf_kernel(X_toy, gamma=1/n_feat)\n",
    "plt.matshow(affinity_matrix)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the similarity graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the `adjacency` matrix of the similarity graph defined by\n",
    "$A_{ij} = \\mathbb{1}_{K_{ij} > \\epsilon}$, for a suitable $\\epsilon$.\n",
    "\n",
    "2. What does this correspond to ? \n",
    "3. Build and display the corresponding similarity graph using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.1\n",
    "adjacency = affinity_matrix > eps\n",
    "plt.matshow(adjacency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_matrix(A=adjacency)\n",
    "nx.draw(G, pos=X_toy,\n",
    "        node_size=1, node_color=cols[y_toy],\n",
    "        width=0.05, edge_color=\"grey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Construct the `laplacian` matrix of the graph defined by\n",
    "$L = D - A$,\n",
    "where\n",
    "- $A$ is the adjacency matrix computed previously\n",
    "- $D$ is the diagonal degree matrix i.e. $D_{ii}=\\sum_{j=1}^N A_{ij}$\n",
    "\n",
    "$L$ is real symmetric, therefore can be diagonalized using `np.linalg.eigh`\n",
    "\n",
    "2. Record the eigenvalues `e_vals` and eigenvectors `e_vecs` and\n",
    "\n",
    "- plot the eigenvalues\n",
    "- plot the 3 first eigenvectors $e_0, e_1, e_2$\n",
    "\n",
    "3. What do you observe ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_degrees = np.diag(adjacency.sum(axis=0))\n",
    "laplacian = diag_degrees - adjacency\n",
    "plt.matshow(laplacian)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the eigenvalues and eigenvectors of the Laplacian matrix, and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_vals, e_vecs = np.linalg.eigh(laplacian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(e_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i in range(3):\n",
    "    ax.plot(e_vecs[:, i], label=r'$e_{}$'.format(i))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these results to perform dimension reduction: perform $k$-means clustering based on the projections of the points on the first axes corresponding to the smallest eigenvalues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the `SpectralClustering` method implemented by `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SpectralClustering(n_clusters=3, affinity='rbf', gamma=1/n_feat)\n",
    "y_pred = sp.fit_predict(X_toy)\n",
    "\n",
    "plt.scatter(X_toy[:,0], X_toy[:,1], c=cols[y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "\n",
    "From [sklearn documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X_mnist = scale(digits.data)\n",
    "n_samples, n_features = X_mnist.shape\n",
    "\n",
    "y_mnist = digits.target\n",
    "n_digits = len(np.unique(y_mnist))\n",
    "\n",
    "cols_mnist = np.array(plt.rcParams['axes.prop_cycle'].by_key()['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size of an image in this dataset ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Type your answer here *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many `n_components` of `PCA` must be kept to capture 95% of the explained variance ?\n",
    "\n",
    "**Hint** Have a look at the documentation of `PCA` and use `.explained_variance_ratio_` and `np.cumsum`.\n",
    "\n",
    "2. Make a suitable plot to visualize it.\n",
    "3. Instanciate a `PCA` with the corresponding `n_components`\n",
    "4. Project the data onto the corresponding lower dimensional subspace using `.transform()` and `.inverse_transform()` methods and display 15 of these approximated images.\n",
    "5. Apply `PCA(n_components=2)` and display the points together with their class.\n",
    "6. Based on this 2D embedding of the data, apply `KMeans` to cluster the points.\n",
    "7. Evaluate this clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.95\n",
    "\n",
    "pca = PCA(n_components=cutoff, svd_solver= 'full').fit(X_mnist)\n",
    "\n",
    "var_ratio = pca.explained_variance_ratio_\n",
    "plt.plot(np.cumsum(var_ratio))\n",
    "plt.hlines(y=cutoff, xmin=0, xmax=pca.explained_variance_.size, label=cutoff)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Answer question 1) *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.title('Cumulative explained variance = f(#components)')\n",
    "ax.plot(np.cumsum(var_ratio))\n",
    "\n",
    "cutoff = 0.95\n",
    "ax.hlines(y=cutoff, xmin=0, xmax=var_ratio.size, label=cutoff)\n",
    "\n",
    "n_comp_95 = np.where(np.cumsum(var_ratio) > 0.95)[0][0]\n",
    "ax.vlines(x=n_comp_95, ymin=0, ymax=1)\n",
    "\n",
    "plt.xlabel('#components')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Here the PCA model, transform and inverse transform of the dataset\n",
    "\n",
    "# pca = ...\n",
    "# X_trans = ...\n",
    "# X_inv_trans = ...\n",
    "\n",
    "### Use the following code to plot the inverse transform after filling\n",
    "# the previous lines\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(8,4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_inv_trans[i].reshape(img_size),\n",
    "              cmap=plt.cm.gray_r, interpolation='nearest',\n",
    "              clim=(0,16))\n",
    "    ax.set(xticks=[], yticks=[],\n",
    "           xlabel='True label: {}'.format(y_mnist[ind_to_show[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the inverse transform of the digits you plotted, how satisfying is it?\n",
    "\n",
    "* Answer*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get good visualizations we are going to test kmeans with a transformed dataset with only two axis. Define below a PCA model with two components and fit it on the mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = \n",
    "# X_mnist_pca = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the K-means part\n",
    "\n",
    "x_min, x_max = X_mnist_pca[:, 0].min() - 1, X_mnist_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_mnist_pca[:, 1].min() - 1, X_mnist_pca[:, 1].max() + 1\n",
    "x1, x2 = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "X_mesh = np.column_stack([x1.ravel(), x2.ravel()])\n",
    "y_mesh = kmeans.predict(X_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define here  a K-Means model with the transformed MNIST dataset\n",
    "\n",
    "# kmeans = ...\n",
    "# kmeans.fit(X_mnist_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Run this cell after filling the previous one \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Display points\n",
    "ax.scatter(X_mnist_pca[:,0], X_mnist_pca[:,1], s=2, c=cols_mnist[y_mnist])\n",
    "# Add text label\n",
    "for (xx, yy), lab in zip(X_mnist_pca, y_mnist):\n",
    "    plt.text(xx, yy, s=str(lab))\n",
    "\n",
    "# Display k-Means centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "ax.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "    \n",
    "# Display boundary decision\n",
    "ax.imshow(y_mesh.reshape(x1.shape), interpolation='nearest',\n",
    "           extent=(x_min, x_max, y_min, y_max),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the results. Does the spatial representation of the numbers make sense? Are some numbers easier to isolate than others, and does this make sense to you? What is the classification error? Plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:08:40.721989Z",
     "start_time": "2020-04-01T10:08:40.719838Z"
    }
   },
   "outputs": [],
   "source": [
    "## Error and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Answer *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of some handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_and_labels = list(zip(digits.images, y_mnist))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "426px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "579px",
    "left": "0px",
    "right": "1098px",
    "top": "110px",
    "width": "162px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
