{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This TP3 is based on the previous TP2 where we considered the `census` dataset and applied the `LogisticRegression`.\n",
    "\n",
    "Prior to that you should have inspected the data.\n",
    "\n",
    "In particular, you may have\n",
    "- used the raw data,\n",
    "- created groups,\n",
    "- or even new features\n",
    "\n",
    "to simplify the learning phase as well as the interpretation of your output.\n",
    "\n",
    "**Based on this important phase of data visualization/inspection/description... we are going to apply some new classification techniques on the same dataset, namely**\n",
    "\n",
    "1. Decision trees through `DecisionTreeClassifier`\n",
    "2. Support Vector Machines through `SVC`\n",
    "\n",
    "**Today, your missions are the following:**\n",
    "\n",
    "1. Practice your basics in the companion notebook `Toy_examples`.\n",
    "2. Create a baseline perfomance on the `census` data with the classifiers of the day.\n",
    "3. Tune the parameters to obtain good classification perfomances using `GridSearchCV`.\n",
    "4. Select between the 2 models based on the classification error on the test set\n",
    "5. Display both the ROC curves and compute the corresponding AUCs.\n",
    "\n",
    "**Hints:**\n",
    "- ðŸ’£ Use notebook `Toy_example` ðŸ’£\n",
    "- Don't forget to `MAJ + Tab*2`!\n",
    "- `sklearn` objects have the same standardized methods as you saw before\n",
    "    - `.fit`\n",
    "    - `.predict/predict_proba`\n",
    "    - `.score` \n",
    "    - ...\n",
    "- You search engine is a nice asset... you can find inspiration in examples of other people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-packages-and-related-objects\" data-toc-modified-id=\"Load-packages-and-related-objects-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load packages and related objects</a></span></li><li><span><a href=\"#Load-and-prepare-the-data\" data-toc-modified-id=\"Load-and-prepare-the-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load and prepare the data</a></span></li><li><span><a href=\"#Vizualization-of-shallow-(low-depth)-trees\" data-toc-modified-id=\"Vizualization-of-shallow-(low-depth)-trees-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Vizualization of shallow (low depth) trees</a></span></li><li><span><a href=\"#Bias-variance-tradeoff\" data-toc-modified-id=\"Bias-variance-tradeoff-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Bias-variance tradeoff</a></span></li><li><span><a href=\"#Optimization-of-the-classifier-with-respect-to-classification-error\" data-toc-modified-id=\"Optimization-of-the-classifier-with-respect-to-classification-error-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Optimization of the classifier with respect to classification error</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preliminary\" data-toc-modified-id=\"Preliminary-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Preliminary</a></span></li><li><span><a href=\"#Let's-practice\" data-toc-modified-id=\"Let's-practice-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Let's practice</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages and related objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use pandas to play with dataset\n",
    "import pandas as pd\n",
    "\n",
    "# use seaborn to display data\n",
    "import seaborn as sns\n",
    "\n",
    "# use sklearn to practice ML\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "\n",
    "# Algorithms of the day\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# To visualize trees \n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the same pipeline as in TP2.\n",
    "\n",
    "But we take a smaller fraction of the data to train the models,\n",
    "mainly because SVMs scale poorly with the size of the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('census.csv', na_values='?')\n",
    "\n",
    "# Drop some sensitive data\n",
    "data.drop(columns=['fnlwgt', 'native.country'], inplace=True)\n",
    "\n",
    "# Drop missing values\n",
    "data.dropna(inplace=True) \n",
    "\n",
    "X_data = data.drop(columns='income', axis=1)\n",
    "\n",
    "# Construct dummy variables for categorical variables\n",
    "categorical_features = ['workclass', 'education', 'marital.status',\n",
    "                        'occupation', 'relationship', 'race', 'sex']\n",
    "X_data = pd.get_dummies(X_data, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Convert string label to 0, 1\n",
    "y_data = LabelEncoder().fit_transform(data['income'])\n",
    "\n",
    "# Split train/test\n",
    "test_frac = 2/3 # Fraction of the data set to consider as test set\n",
    "\n",
    "X_train, X_test,\\\n",
    "y_train, y_test = train_test_split(X_data, y_data,\n",
    "                                   test_size=test_frac,\n",
    "                                   shuffle=True, # Shuffle the data points before split\n",
    "                                   stratify=y_data, # Respect the proportion of classes\n",
    "                                   random_state=123) # To compare your performance to your neighbor's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to Logistic Regression\n",
    "\n",
    "Here you are simply given an example of code to:\n",
    "\n",
    "- instanciate the model with `sklearn`\n",
    "- build the corresponding ROC curve and compute the corresponding AUC\n",
    "- plot the ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rember that the underlying optimization problem solved by `sklearn` reads in this case\n",
    "\n",
    "$$\n",
    "\\min_{w, c} \\frac{1}{2}w^T w + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1)\n",
    "$$\n",
    "\n",
    "so you can take $C\\gg 1$ to work as if there was no regularization: the version of `LogisticRegression` you were taught in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Instanciate/train/score the model\n",
    "\n",
    "a. Choose 3 parameters of the `LogisticRegression` object and describe them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C=1e5)\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "test_accuracy = log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Build the Receiver Operating Characteristic curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Describe in a few words how the ROC curve is constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: predict the probability to be classified as 1\n",
    "proba_pred_1 = log_reg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# 2: compute false/true positive rates obtained by moving the detection threshold\n",
    "fpr, tpr, tresh = roc_curve(y_test, proba_pred_1)\n",
    "\n",
    "# Compute the Area Under the Curve\n",
    "eval_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plot the ROC curve\n",
    "\n",
    "c. Comment on the shape of the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# Plot the ROC curve\n",
    "lab = 'LogisticRegression: test accuracy={:.2f}% AUC={:.2f}'.format(test_accuracy, eval_auc)\n",
    "ax.plot(fpr, tpr,\n",
    "        lw=2, \n",
    "        label=lab) \n",
    "\n",
    "# Plot the ROC curve associated to a coin flip\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.01])\n",
    "\n",
    "plt.legend(loc='best')\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines for this TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good starting point to establish a baseline to know what performance you should beat.\n",
    "\n",
    "For this you can naively instanciate the different classifiers by using the default parameters.\n",
    "\n",
    "Let's do this for `LogisticRegression` and `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "log_reg = LogisticRegression(C=1e5)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# classification score\n",
    "log_reg_test_acc = log_reg.score(X_test, y_test)\n",
    "\n",
    "# ROC/AUC\n",
    "proba_pred_1 = log_reg.predict_proba(X_test)[:,1]\n",
    "log_reg_fpr, log_reg_tpr, _ = roc_curve(y_test, proba_pred_1)\n",
    "log_reg_auc = auc(log_reg_fpr, log_reg_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# classification score\n",
    "tree_test_acc = tree.score(X_test, y_test)\n",
    "\n",
    "# ROC/AUC\n",
    "proba_pred_1 = tree.predict_proba(X_test)[:,1]\n",
    "tree_fpr, tree_tpr, _ = roc_curve(y_test, proba_pred_1)\n",
    "tree_auc = auc(tree_fpr, tree_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# ROC curves\n",
    "lab = 'LogisticRegression: test accuracy={:.2f}% AUC={:.2f}'.format(log_reg_test_acc, log_reg_auc)\n",
    "ax.plot(log_reg_fpr, log_reg_tpr,\n",
    "        lw=2, \n",
    "        label=lab) \n",
    "\n",
    "# ROC curves\n",
    "lab = 'LogisticRegression: test accuracy={:.2f}% AUC={:.2f}'.format(tree_test_acc, tree_auc)\n",
    "ax.plot(tree_fpr, tree_tpr,\n",
    "        lw=2, \n",
    "        label=lab) \n",
    "\n",
    "# Plot the ROC curve associated to a coin flip\n",
    "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0.0, 1.01])\n",
    "\n",
    "plt.legend(loc='best')\n",
    "          \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Explain how a ROC curve is computed.\n",
    "2. What can the ROC curve be used for ?\n",
    "\n",
    "**Bonus:** \n",
    "1. Think about a more compact way to reproduce the same plot (a loop might be a starting point).\n",
    "2. You can also superimpose the results for `SVC`. Beware, the training phase may take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "In this section, we focus on the `DecisionTreeClassifier` and strive to optimize its performances on our `census` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualization of shallow (low depth) trees\n",
    "\n",
    "Consider `DecisionTreeClassifier(max_depth=)`\n",
    "\n",
    "**Questions**\n",
    "\n",
    "1. Display the trees corresponding to `max_depth` parameters in $\\{1, 2, 3\\}$ (have a look at the `Toy_Example` notebook).\n",
    "\n",
    "2. Comment on these observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-variance tradeoff \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider `DecisionTreeClassifier(max_depth=)`\n",
    "\n",
    "**Questions**\n",
    "\n",
    "For each `max_depth` $\\in\\{1,\\dots, 15\\}$.\n",
    "\n",
    "1. Train the model on the full training set.\n",
    "2. Save the corresponding training and test errors.\n",
    "3. On a single plot, display the evolution of these errors with respect to `max_depth`.\n",
    "4. Comment on the output plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization of the classifier with respect to classification error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the very convenient `GridSearchCV` method to tune our `DecisionTreeClassifier` in order improve our baseline.\n",
    "\n",
    "1. Explain in a few words the purpose of cross validation and how it works.\n",
    "\n",
    "As you should know by now there are many parameters that can be tuned to get better performance.\n",
    "\n",
    "2. Take 3 parameters of the `DecisionTreeClassifier` and give their role.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by optimizing `max_depth` alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'max_depth': np.arange(3, 20)}\n",
    "cv = 5\n",
    "scoring_rule = 'accuracy'\n",
    "tree_grid = GridSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                         param_grid=grid,\n",
    "                         cv=cv,\n",
    "                         scoring=scoring_rule,\n",
    "                         return_train_score=True)\n",
    "\n",
    "tree_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the attribute `tree_grid.cv_results_`\n",
    "\n",
    "1. On the same plot, display the test and train cross-validation errors w.r.t. `max_depth`\n",
    "2. What is the best value of `max_depth` in this case ?\n",
    "3. Visualize the optimized decision tree.\n",
    "4. Comment on these observations.\n",
    "\n",
    "Step up to the next level by tuning multiple parameters at the same time with no extra effort!\n",
    "\n",
    "5. In the same spirit, investigate different parameters and tune them separately.\n",
    "6. Save the resulting classifier as `tree_opti_separate` and compute its error on the test set.\n",
    "7. Use `GridSearchCV` to build the correponding grid of parameters \n",
    "8. Extract the best classifier associated to this grid of parameters and save it as `tree_opti_joint` (have a look at the `best_estimator_` attribute).\n",
    "9. Give its performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "With your own words, answer the following questions\n",
    "1. What is a kernel for you?\n",
    "2. In what context have you already met the kernel defined by $K(x,y)=\\min(x,y)$?\n",
    "3. What to you know about SVMs? (objective function, kernel trick, dual problem...)\n",
    "\n",
    "Next, we turn to the optimization of the SVM.\n",
    "\n",
    "4. Take 3 parameters of `SVC` and give their role.\n",
    "5. Apply the recipe used for `DecisionTreeClassifier` to come up with an optimized SVM classifier.\n",
    "\n",
    "**Warning** training a SVM takes time, it scales as $\\mathcal{O}(N^3)$ where $N$ is the number of training examples. If this is really too loog you can reconsider the train/test split by consider a small fraction for the **training** set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare the performance of your optimized classifier to the baselines established in Section 3.\n",
    "\n",
    "- Which classifier would you choose and why ?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "426px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "579px",
    "left": "0px",
    "right": "1098px",
    "top": "110px",
    "width": "162px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
